{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40682a80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# Enable gensim logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "\n",
    "class W2VLossLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to print loss after each epoch\n",
    "    use by passing model.train(..., callbacks=[W2VLossLogger()])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "\n",
    "        if self.epoch == 0:\n",
    "            print(\"Loss after epoch {}: {}\".format(self.epoch, loss))\n",
    "        else:\n",
    "            print(\n",
    "                \"Loss after epoch {}: {}\".format(\n",
    "                    self.epoch, loss - self.loss_previous_step\n",
    "                )\n",
    "            )\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n",
    "\n",
    "def train_w2v_model(\n",
    "    sentences,\n",
    "    output_file,\n",
    "    window,\n",
    "    embedding_dim,\n",
    "    epochs,\n",
    "    min_word_count,\n",
    "):\n",
    "    \n",
    "\n",
    "    \"\"\"Train a word2vec model based on given sentences.\n",
    "    Args:\n",
    "        sentences list[list[str]]: List of sentences. Each element contains a list with the words\n",
    "            in the current sentence\n",
    "        output_file (str): Path to save the trained w2v model\n",
    "        window (int): w2v context size\n",
    "        embedding_dim (int): w2v vector dimension\n",
    "        epochs (int): How many epochs should the training run\n",
    "        min_word_count (int): Ignore words that appear less than min_word_count times\n",
    "    \"\"\"\n",
    "    workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    # TODO: Instantiate gensim.models.Word2Vec class\n",
    "    model = Word2Vec(sentences=sentences, vector_size=embedding_dim, window=window, min_count=min_word_count, workers=multiprocessing.cpu_count())\n",
    "    model.build_vocab(sentences, progress_per=10000)\n",
    "    # TODO: Build model vocabulary using sentences\n",
    "    # TODO: Train word2vec model\n",
    "    model.train(sentences, total_examples=model.corpus_count,epochs=epochs)\n",
    "    # Save trained model\n",
    "    model.save(output_file)\n",
    "    # model.save(output_file)\n",
    "\n",
    "    \n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # read data/gutenberg.txt in the expected format (tokenized)\n",
    "    f=open(\"../data/tokenized.txt\",\"r\")\n",
    "    sentences =eval(f.read())\n",
    "    \n",
    "   \n",
    "    output_file = \"gutenberg_w2v.hundd.model\"\n",
    "    window = 5\n",
    "    embedding_dim = 100\n",
    "    epochs = 1000\n",
    "    min_word_count = 1\n",
    "\n",
    "    \n",
    "    #Initialize training of our Word2Vec model\n",
    "    \n",
    "    gutenberg_w2v =train_w2v_model(\n",
    "        sentences,\n",
    "        output_file,\n",
    "        window,\n",
    "        embedding_dim,\n",
    "        epochs,\n",
    "        min_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "gutenberg_w2v = Word2Vec.load(\"gutenberg_w2v.hundd.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8718d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_w2v.wv.most_similar([\"bible\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_w2v.wv.most_similar([\"book\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_w2v.wv.most_similar([\"bank\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_w2v.wv.most_similar([\"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba415a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = gutenberg_w2v.wv[\"good\"] - gutenberg_w2v.wv[\"taller\"] + gutenberg_w2v.wv[\"tall\"]\n",
    "gutenberg_w2v.wv.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = gutenberg_w2v.wv[\"girls\"] - gutenberg_w2v.wv[\"queen\"] + gutenberg_w2v.wv[\"kings\"]\n",
    "gutenberg_w2v.wv.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = gutenberg_w2v.wv[\"france\"] - gutenberg_w2v.wv[\"paris\"] + gutenberg_w2v.wv[\"london\"]\n",
    "gutenberg_w2v.wv.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "google_model = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin.gz', binary=True,\n",
    "limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar([\"bible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar([\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar([\"bank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model.most_similar([\"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eca520",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = google_model[\"girls\"] - google_model[\"queen\"] + google_model[\"kings\"]\n",
    "google_model.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e80310",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = google_model[\"good\"] - google_model[\"taller\"] + google_model[\"tall\"]\n",
    "google_model.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = google_model[\"france\"] - google_model[\"paris\"] + google_model[\"london\"]\n",
    "google_model.most_similar(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(voc), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(voc)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index_to_key[i]] \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embeddings=to_embeddings_Matrix(gutenberg_w2v)\n",
    "print(np.shape(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1945bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in data folder the embeddings and the metadata to load into the data visualization tool\n",
    "\n",
    "import csv\n",
    "with open('../data/embeddings.tsv', 'w', newline='') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "    for embedding in embeddings:\n",
    "        tsv_output.writerow(embedding)\n",
    "    \n",
    "with open('../data/metadata.tsv', 'w', newline='') as f_output:\n",
    "    tsv_output = csv.writer(f_output)\n",
    "    for voc_rows in voc:\n",
    "        tsv_output.writerow([voc_rows])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "SCRIPT_DIRECTORY = os.path.realpath(os.getcwd())\n",
    "\n",
    "data_dir = os.path.join(SCRIPT_DIRECTORY, \"../data/aclImdb\")\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "pos_train_dir = os.path.join(train_dir, \"pos\")\n",
    "neg_train_dir = os.path.join(train_dir, \"neg\")\n",
    "pos_test_dir = os.path.join(test_dir, \"pos\")\n",
    "neg_test_dir = os.path.join(test_dir, \"neg\")\n",
    "\n",
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "MAX_NUM_SAMPLES = 5000\n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 1000000\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub(\"\\s+\", \" \", strip_punctuation(s).lower())\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(\" \")\n",
    "\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))\n",
    "\n",
    "\n",
    "# Preprocess and tokenize the reviews, it will come out as list of lists \n",
    "def token_proc(t_corpus):\n",
    "    data=[]\n",
    "    for i,ind in enumerate(t_corpus):\n",
    "        proc_t_corpus=preproc_tok(train_corpus[i])\n",
    "        data.append(proc_t_corpus)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, \"*.txt\"))\n",
    "    data = []\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, \"r\") as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg)\n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return list(corpus[indices]), list(y[indices])\n",
    "\n",
    "\n",
    "def extract_nbow(model,train_data,test_data):\n",
    "    \"\"\"Extract neural bag of words representations\"\"\"\n",
    "    \n",
    "    # The training dataset (sentences of the reviews) will be converted to vectors of 100 dimensions \n",
    "    X_train = np.zeros((np.size(train_data), 100))\n",
    "    for row, rev in enumerate(train_data):\n",
    "        words_included = 0\n",
    "\n",
    "        # Tokenize current review\n",
    "        rev_toks = preproc_tok(rev)\n",
    "    \n",
    "        for tok in rev_toks:\n",
    "            if tok in model.wv:\n",
    "                X_train[row] += model.wv[tok]\n",
    "                words_included += 1\n",
    "            \n",
    "        # Get the mean value of each sentence in the embedding space\n",
    "        X_train[row] = X_train[row]/words_included\n",
    "\n",
    "\n",
    "\n",
    "    # The test dataset (sentences of the reviews) will be converted to vectors of 100 dimensions \n",
    "    X_test = np.zeros((np.size(test_data), 100)) \n",
    "    for row, rev in enumerate(test_data):\n",
    "        words_included = 0\n",
    "        \n",
    "        # Tokenize current review\n",
    "        rev_toks = preproc_tok(rev)\n",
    "        for tok in rev_toks:\n",
    "            # For each token check if it has a w2v representation\n",
    "            # and if yes add it.\n",
    "            if tok in model.wv:\n",
    "                X_test[row] += model.wv[tok]\n",
    "                words_included += 1\n",
    "                \n",
    "        # Get the mean value of each sentence in the embedding space\n",
    "        X_test[row] = X_test[row]/words_included\n",
    "\n",
    "    return X_train,X_test\n",
    "\n",
    "    raise NotImplementedError(\"Implement nbow extractor\")\n",
    "\n",
    "\n",
    "# def train_sentiment_analysis(train_corpus, train_labels):\n",
    "#     \"\"\"Train a sentiment analysis classifier using NBOW + Logistic regression\"\"\"\n",
    "#     raise NotImplementedError(\"Implement sentiment analysis training\")\n",
    "\n",
    "\n",
    "# def evaluate_sentiment_analysis(classifier, test_corpus, test_labels):\n",
    "#     \"\"\"Evaluate classifier in the test corpus and report accuracy\"\"\"\n",
    "#     raise NotImplementedError(\"Implement sentiment analysis evaluation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454150e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Positive and negative reviews train dataset\n",
    "# pos_train=read_samples(pos_train_dir)\n",
    "# neg_train=read_samples(neg_train_dir)\n",
    "\n",
    "# # Positive and negative reviews test dataset\n",
    "# pos_test=read_samples(pos_test_dir)\n",
    "# neg_test=read_samples(neg_test_dir)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e666e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok_corpus=[]\n",
    "proc_tok_rev=[]\n",
    "\n",
    "for rev in corpus:\n",
    "    proc_tok_rev=preproc_tok(rev)\n",
    "\n",
    "    proc_tok_corpus.append(proc_tok_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea678c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "train_w2v_model(proc_tok_corpus,\n",
    "   \"my_sentiment_w2v.model\",\n",
    "    5,\n",
    "    100,\n",
    "    1000,\n",
    "    1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "my_sentiment_w2v = Word2Vec.load(\"my_sentiment_w2v.model\")\n",
    "\n",
    "\n",
    "# Convert to numpy 2d array (n_vocab x vector_size)\n",
    "def to_embeddings_Matrix(model):  \n",
    "    embedding_matrix = np.zeros((len(voc), model.vector_size))\n",
    "    word2idx = {}\n",
    "    for i in range(len(voc)):\n",
    "        embedding_matrix[i] = model.wv[model.wv.index_to_key[i]] \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embeddings_my_sentiment=to_embeddings_Matrix(my_sentiment_w2v)\n",
    "\n",
    "print(np.shape(embeddings_my_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25847524",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels = create_corpus(read_samples(pos_train_dir), read_samples(neg_train_dir))\n",
    "test_data,test_labels = create_corpus(read_samples(pos_test_dir), read_samples(neg_test_dir))\n",
    "\n",
    "\n",
    "X_train,X_test=extract_nbow(my_sentiment_w2v,train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e756103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf=LogisticRegression().fit(X_train,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_pred,test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "google_sentiment_w2v = KeyedVectors.load_word2vec_format('/home/brewed/Desktop/GoogleNews-vectors-negative300.bin.gz', binary=True,\n",
    "limit=1000000)\n",
    "\n",
    "embeddings_google_sentiment = np.zeros((len(voc), google_sentiment_w2v.vector_size))\n",
    "word2idx = {}\n",
    "for i in range(len(voc)):\n",
    "    embeddings_google_sentiment[i] = google_sentiment_w2v[google_sentiment_w2v.index_to_key[i]] \n",
    "\n",
    "print(np.shape(embeddings_google_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels=create_corpus(read_samples(pos_train_dir), read_samples(neg_train_dir))\n",
    "test_data,test_labels= create_corpus(read_samples(pos_test_dir), read_samples(neg_test_dir))\n",
    "\n",
    "\n",
    "X_google_train = np.zeros((np.size(train_data), 300))\n",
    "for row, rev in enumerate(train_data):\n",
    "    words_included = 0\n",
    "    rev_toks = preproc_tok(rev)\n",
    "    \n",
    "    for tok in rev_toks:\n",
    "        if tok in google_sentiment_w2v:\n",
    "            X_google_train[row] += google_sentiment_w2v[tok]\n",
    "            words_included += 1\n",
    "\n",
    "    # Get the mean value\n",
    "    X_google_train[row] = X_google_train[row]/words_included\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_google_test = np.zeros((np.size(test_data), 300)) \n",
    "for row, rev in enumerate(test_data):\n",
    "    words_included = 0\n",
    "    # Tokenize current review\n",
    "    rev_toks = preproc_tok(rev)\n",
    "    for tok in rev_toks:\n",
    "        # For each token check if it has a w2v representation\n",
    "        # and if yes add it.\n",
    "        if tok in google_sentiment_w2v:\n",
    "            X_google_test[row] += google_sentiment_w2v[tok]\n",
    "            words_included += 1\n",
    "    # Get the mean value\n",
    "    X_google_test[row] = X_google_test[row]/words_included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf=LogisticRegression().fit(X_google_train,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X_google_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_pred,test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeedf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
